<h1 align="center">
  <img
    src="gallery/banner.jpg"
    alt="Hi, JAX! An introduction to JAX for deep learning researchers"
    title="(Image generated by Nano Banana Pro)"
  />
</h1>

This is the codebase for "Hi, JAX!", an introduction to
  [JAX](https://jax.readthedocs.io/)
for deep learning researchers.

The code examples and recording of video lectures are a work in progress:

* Code examples: 11/16.
* Video lectures: 0/16.

For full syllabus information, see the
  [course website](https://far.in.net/hijax).

<!--TODO: Link lecture recordings.-->

<table>
<tbody>
  <tr>
    <td align="center"><img src="gallery/lecture00.png" height="156px"><br>Lecture 00</td>
    <td align="center"><img src="gallery/lecture01.gif" height="156px"><br>Lecture 01</td>
    <td align="center"><img src="gallery/lecture02.gif" height="156px"><br>Lecture 02</td>
    <td align="center"><img src="gallery/lecture03.gif" height="156px"><br>Lecture 03</td>
  </tr>
  <tr>
    <td align="center"><img src="gallery/lecture04.gif" height="156px"><br>Lecture 04</td>
    <td align="center"><img src="gallery/lecture05.gif" height="156px"><br>Lecture 05</td>
    <td align="center"><img src="gallery/lecture06.gif" height="156px"><br>Lecture 06</td>
    <td align="center"><img src="gallery/lecture07.gif" height="156px"><br>Lecture 07</td>
  </tr>
  <tr>
    <td align="center"><img src="gallery/lecture08.gif" height="156px"><br>Lecture 08</td>
    <td align="center"><img src="gallery/lecture09.gif" height="156px"><br>Lecture 09</td>
    <td align="center"><img src="gallery/lecture10.gif" height="156px"><br>Lecture 10</td>
    <td align="center"><img src="gallery/placeholder2.jpg" height="156px"><br>Lecture 11</td>
  </tr>
</tbody>
</table>

*Note:
  This page is for the online edition of Hi, JAX! See also:
    [2024 edition](hijax-2024).
  Visualisations/animations made with
    [matthewplotlib](https://github.com/matomatical/matthewplotlib).
  Banner image by Nano Banana Pro.*

Prerequisites
-------------

Programming:

* Prior experience programming in Python
  (e.g., collections, dataclasses, functools, type annotations).
* Prior experience programming in NumPy.
  * NumPy [basics](https://numpy.org/doc/stable/user/absolute_beginners.html)
    tutorial should be sufficient.
* Prior experience with einops.
  * Einops [basics](https://einops.rocks/1-einops-basics/)
    tutorial more than sufficient.

Machine learning:

* Basic vector calculus and optimisation (stochastic gradient descent).
* Basic deep learning architectures (MLP, CNN, Transformer).
  * [3Blue1Brown Season 3](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
    should be sufficient (skip chapter 4).

*Helpful/optional (we will cover what is required):*

* Basic reinforcement learning
    (Markov decision process formalism, policy gradients).
* Prior experience programming in, e.g., Rust
    (the concept of immutability).
* Prior experience programming in, e.g., Haskell
    (the concepts of pure functions, mapping, folding).

You will need a Python environment (but not necessarily a GPU) if you want to
code along during the workshops.


Syllabus
--------

For full syllabus information, see the
  [course website](https://far.in.net/hijax).

### Overture

In which we first meet JAX and get a taste of how it differs from NumPy.

0.  **Hi, JAX! How's life?**
    Quick overview of JAX features, `jax.numpy` library.

### Act I: Basics

In which we learn the elementary components of JAX programs while implementing
and training increasingly complex neural networks.

1.  **Hi, automatic differentiation!**
    Functional model API, `jax.grad` transformation.

2.  **Hi, procedural random number generation!**
    Immutable PRNG state management, `jax.random` library.

3.  **Hi, PyTrees!**
    PyTrees, `jax.tree.map`.

4.  **Hi, automatic vectorisation!**
    Vectorisation with `jax.vmap`.

5.  **Hi, stateful optimisation!**
    Managing state during a training loop.

### Act II: Acceleration

In which we explore various aspects of just-in-time compilation and the kinds
of tricks we need to use to prepare our computational graphs for the XLA
compiler.

6.  **Hi, just-in-time compilation!**
    Compilation with `jax.jit`, tracing versus execution, side-effects,
    debugging tools.

7.  **Hi, loop acceleration!**
    Looping computations with `jax.lax.scan`.

8.  **Hi, static arguments!**
    Compile errors due to non-static shapes, flagging static arguments.

9.  **Hi, branching computation!**
    Stateful environment API, conditional computation with `jax.lax.select`,
    `jax.numpy.where`, and expression-level branching.
    
10. **Hi, algorithms!**
    Performance considerations for branching computation and parallelism,
    `jax.lax.while`.
    
### Finale

In which we bring together everything we have learned to accelerate an
end-to-end deep reinforcement learning environment simulation and training
loop, one of the most effective uses of JAX for deep learning.

11. **Hi, deep reinforcement learning!**
    Revision of previous fundamental topics, reverse `scan`.
    
Getting started
---------------

Create a virtual environment, for example with
  [uv](https://docs.astral.sh/uv/getting-started/installation/#pypi):

```
uv venv hijax.venv
```

Enter the virtual environment:

```
source hijax.venv/bin/activate
```

Install the requirements:

```
pip install -r requirements.txt
```

Note: That command will install JAX to use the CPU.

* If you want to run JAX on a GPU with CUDA 13: `pip install jax[cuda13]`
* If you want to run JAX on a TPU: `pip install jax[tpu]`
* More generally see the
  [JAX installation instructions](https://docs.jax.dev/en/latest/installation.html)

From there, you should be able to follow along with each video tutorial.
